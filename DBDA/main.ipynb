{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "from torch import optim\n",
    "import torch\n",
    "from sklearn import metrics, preprocessing\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('/root/DBDA/global_module/')\n",
    "\n",
    "import network\n",
    "import train\n",
    "from generate_pic import aa_and_each_accuracy, sampling, load_dataset, generate_png, generate_iter\n",
    "from Utils import fdssc_model, record, extract_samll_cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Importing Dataset-----\n",
      "(610, 340, 103)\n",
      "The class numbers of the HSI data is: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((207400, 103), (207400,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# for Monte Carlo runs\n",
    "seeds = [1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341]\n",
    "ensemble = 1\n",
    "\n",
    "day = datetime.datetime.now()\n",
    "day_str = day.strftime('%m_%d_%H_%M')\n",
    "IMAGE_FOLDER = '/root/DBDA/DBDA/records/figures'\n",
    "\n",
    "print('-----Importing Dataset-----')\n",
    "\n",
    "\n",
    "\n",
    "global Dataset  # UP,IN,KSC\n",
    "dataset_str = input('Please input the name of Dataset(IN, UP, BS, SV, PC, DN, DN_1, WHL, HC, HH or KSC):')\n",
    "Dataset = dataset_str.upper()\n",
    "data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE,VALIDATION_SPLIT = load_dataset(Dataset)\n",
    "\n",
    "print(data_hsi.shape)\n",
    "image_x, image_y, BAND = data_hsi.shape\n",
    "data = data_hsi.reshape(np.prod(data_hsi.shape[:2]), np.prod(data_hsi.shape[2:]))\n",
    "gt = gt_hsi.reshape(np.prod(gt_hsi.shape[:2]),)\n",
    "CLASSES_NUM = max(gt)\n",
    "print('The class numbers of the HSI data is:', CLASSES_NUM)\n",
    "data.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-----Importing Setting Parameters-----')\n",
    "ITER = 10\n",
    "PATCH_LENGTH = 4\n",
    "# number of training samples per class\n",
    "#lr, num_epochs, batch_size = 0.001, 200, 32\n",
    "lr, num_epochs, batch_size = 0.00050, 200, 16\n",
    "#lr, num_epochs, batch_size = 0.0005, 200, 12\n",
    "#net = network.DBDA_network_drop(BAND, CLASSES_NUM)\n",
    "#net = network.DBDA_network_PReLU(BAND, CLASSES_NUM)\n",
    "# net = network.DBMA_network(BAND, CLASSES_NUM)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=lr) #, weight_decay=0.0001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "img_rows = 2*PATCH_LENGTH+1\n",
    "img_cols = 2*PATCH_LENGTH+1\n",
    "img_channels = data_hsi.shape[2]\n",
    "INPUT_DIMENSION = data_hsi.shape[2]\n",
    "ALL_SIZE = data_hsi.shape[0] * data_hsi.shape[1]\n",
    "VAL_SIZE = int(TRAIN_SIZE)\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "\n",
    "\n",
    "KAPPA = []\n",
    "OA = []\n",
    "AA = []\n",
    "TRAINING_TIME = []\n",
    "TESTING_TIME = []\n",
    "ELEMENT_ACC = np.zeros((ITER, CLASSES_NUM))\n",
    "\n",
    "data = preprocessing.scale(data)\n",
    "data_ = data.reshape(data_hsi.shape[0], data_hsi.shape[1], data_hsi.shape[2])\n",
    "whole_data = data_\n",
    "padded_data = np.lib.pad(whole_data, ((PATCH_LENGTH, PATCH_LENGTH), (PATCH_LENGTH, PATCH_LENGTH), (0, 0)),\n",
    "                         'constant', constant_values=0)\n",
    "\n",
    "for index_iter in range(ITER):\n",
    "    print('iter:', index_iter)\n",
    "    net = network.DBDA_network_MISH(BAND, CLASSES_NUM)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, amsgrad=False) #, weight_decay=0.0001)\n",
    "    time_1 = int(time.time())\n",
    "    np.random.seed(seeds[index_iter])\n",
    "    train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "    _, total_indices = sampling(1, gt)\n",
    "\n",
    "    TRAIN_SIZE = len(train_indices)\n",
    "    print('Train size: ', TRAIN_SIZE)\n",
    "    TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "    print('Test size: ', TEST_SIZE)\n",
    "    VAL_SIZE = int(TRAIN_SIZE)\n",
    "    print('Validation size: ', VAL_SIZE)\n",
    "\n",
    "    print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
    "\n",
    "    train_iter, valida_iter, test_iter, all_iter = generate_iter(TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE, total_indices, VAL_SIZE,\n",
    "                  whole_data, PATCH_LENGTH, padded_data, INPUT_DIMENSION, batch_size, gt)\n",
    "\n",
    "    tic1 = time.perf_counter()\n",
    "    train.train(net, train_iter, valida_iter, loss, optimizer, device, dataset_str,IMAGE_FOLDER,index_iter,epochs=num_epochs)\n",
    "    toc1 = time.perf_counter()\n",
    "\n",
    "    pred_test_fdssc = []\n",
    "    tic2 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            X = X.to(device)\n",
    "            net.eval()  # 评估模式, 这会关闭dropout\n",
    "            y_hat = net(X)\n",
    "            pred_test_fdssc.extend(np.array(net(X).cpu().argmax(axis=1)))\n",
    "    toc2 = time.perf_counter()\n",
    "    collections.Counter(pred_test_fdssc)\n",
    "    gt_test = gt[test_indices] - 1\n",
    "\n",
    "\n",
    "    overall_acc_fdssc = metrics.accuracy_score(pred_test_fdssc, gt_test[:-VAL_SIZE])\n",
    "    confusion_matrix_fdssc = metrics.confusion_matrix(pred_test_fdssc, gt_test[:-VAL_SIZE])\n",
    "    each_acc_fdssc, average_acc_fdssc = aa_and_each_accuracy(confusion_matrix_fdssc)\n",
    "    kappa = metrics.cohen_kappa_score(pred_test_fdssc, gt_test[:-VAL_SIZE])\n",
    "\n",
    "    torch.save(net.state_dict(), \"/root/DBDA/DBDA/pt/\" + str(round(overall_acc_fdssc, 3)) + '.pt')\n",
    "    KAPPA.append(kappa)\n",
    "    OA.append(overall_acc_fdssc)\n",
    "    AA.append(average_acc_fdssc)\n",
    "    TRAINING_TIME.append(toc1 - tic1)\n",
    "    TESTING_TIME.append(toc2 - tic2)\n",
    "    ELEMENT_ACC[index_iter, :] = each_acc_fdssc\n",
    "\n",
    "print(\"--------\" + net.name + \" Training Finished-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.record_output(OA, AA, KAPPA, ELEMENT_ACC, TRAINING_TIME, TESTING_TIME,\n",
    "                     '/root/DBDA/DBDA/records/log/' + net.name + day_str + '_' + Dataset + 'split：' + str(VALIDATION_SPLIT) + 'lr：' + str(lr) + '.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_png(all_iter, net, gt_hsi, Dataset, device, total_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
